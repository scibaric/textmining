{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd06db01c7983bb179387c9cb4b493325ca9048c8382ce5234b73d81be46f221dc1",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "with open('2020.txt', 'r', encoding = \"UTF-8\") as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "source": [
    "### Tokeniziranje teksta"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(data)\n",
    "print(\"Tokenizirani tekst\\n\")\n",
    "print(tokens)\n",
    "print(\"Broj tokena\")"
   ]
  },
  {
   "source": [
    "### Tokeniziranje teksta na nacin da vrati samo rijeci bez interpunkcijskih znakova"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(data)\n",
    "print(\"Tokenizirani tekst bez interpunkcijskih znakova\\n\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Rijeci s velikim slovima pretvaramo u mala slova kako bi mogli lakse maknuti stop wordove"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = [word.lower() for word in tokens]\n",
    "fresh_tokens = list(tokens)\n",
    "\n",
    "fresh_tokens_without_stop_words = []\n",
    "for word in fresh_tokens:\n",
    "    if word not in set(stopwords.words(\"english\")):\n",
    "        fresh_tokens_without_stop_words.append(word)\n",
    "\n",
    "word_count = Counter(fresh_tokens_without_stop_words)\n",
    "print(\"Najfrekventnije riječi prije lematizacije i stematizacije\")\n",
    "print(word_count.most_common(30))\n",
    "\n",
    "fd = FreqDist(fresh_tokens_without_stop_words)\n",
    "fd.plot(30, title='30 najfrekventnijih riječi prije lematizacije i stematizacije')"
   ]
  },
  {
   "source": [
    "### Stematizacija - PorterStemmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = []\n",
    "\n",
    "for word in tokens:\n",
    "    stemmed_tokens.append(porter_stemmer.stem(word))\n",
    "\n",
    "print(\"Stematizirani tekst\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "source": [
    "### Lematizacija"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = []\n",
    "\n",
    "for word in stemmed_tokens:\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "print(\"Lematizirani tekst\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "source": [
    "### Najfrekventije riječi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(lemmatized_tokens)\n",
    "print(\"Najfrekventnije riječi \", word_count.most_common(30))\n",
    "\n",
    "fd = FreqDist(lemmatized_tokens)\n",
    "fd.plot(30, title='30 najfrekventnijih riječi')\n"
   ]
  },
  {
   "source": [
    "### Najfrekventije riječi bez stop word-ova"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "tokens_without_stop_words = []\n",
    "\n",
    "for word in lemmatized_tokens:\n",
    "    if word not in nltk_stop_words:\n",
    "        tokens_without_stop_words.append(word)\n",
    "\n",
    "word_count_without_stop_words = Counter(tokens_without_stop_words)\n",
    "print(\"Najfrekventije riječi bez stop word-ova\")\n",
    "print(word_count_without_stop_words.most_common(30))\n",
    "fd = FreqDist(word_count_without_stop_words)\n",
    "fd.plot(30, title='30 najfrekventijih riječi bez stop word-ova')"
   ]
  },
  {
   "source": [
    "### Concordance 10 najčešćih riječi u tekstu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "\n",
    "ten_most_commont_words = word_count_without_stop_words.most_common(10)\n",
    "print(ten_most_commont_words)\n",
    "for w in ten_most_commont_words:\n",
    "    text.concordance(w[0])"
   ]
  },
  {
   "source": [
    "### Kolokacije 10 najčešćih riječi"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "bigram_finder.apply_freq_filter(2)\n",
    "print(\"Bigrami\")\n",
    "print(bigram_finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "bigram_finder.apply_word_filter(lambda w: 'it' in w or 'be' in w or 'didn' in w or 'month' in w \n",
    "or 'croatian' in w or 'palma' in w or 'alen' in w or 'dinamo' in w)\n",
    "print(\"Bigrami s filterom\")\n",
    "print(bigram_finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "\n",
    "\n",
    "trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "trigram_finder.apply_freq_filter(2)\n",
    "print(\"Trigrami\")\n",
    "print(trigram_finder.nbest(trigram_measures.likelihood_ratio, 10))\n",
    "trigram_finder.apply_word_filter(lambda w: 'month'in w or 'croatian' in w)\n",
    "print(\"Trigrami s filterom\")\n",
    "print(trigram_finder.nbest(trigram_measures.likelihood_ratio, 10))\n"
   ]
  },
  {
   "source": [
    "### Leksicki diverzificitet i postotak najcesce rijeci u tekstu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "def percentage(count, total):\n",
    "    return 100 * count / total\n",
    "\n",
    "print('Leksicki diverzificitet ', lexical_diversity(lemmatized_tokens))\n",
    "print('Postotak rijeci halilov u tekstu ', round(percentage(lemmatized_tokens.count('halilov'), len(lemmatized_tokens)), 2))"
   ]
  },
  {
   "source": [
    "### Korpus teksta Brown"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "print('Brown kategorije:\\n', brown.categories())\n",
    "genres = brown.categories()\n",
    "words = ['croatian', 'player', 'deal', 'loan', 'barça', 'career', 'debut', 'palmas',  'birmingham']\n",
    "most_frequent_words_in_brown = {}\n",
    "for genre in genres:\n",
    "    genre_text = brown.words(categories = genre)\n",
    "    fdist = nltk.FreqDist(genre_text)\n",
    "    for w in words:\n",
    "        if fdist[w] > 0:\n",
    "            lst = []\n",
    "            lst.append(w)\n",
    "            lst.append(fdist[w])\n",
    "            most_frequent_words_in_brown[genre] = lst\n",
    "\n",
    "print()\n",
    "for key in most_frequent_words_in_brown:\n",
    "    print(key, '->', most_frequent_words_in_brown[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "\n",
    "print('Reuters kategorije:\\n', reuters.categories())\n",
    "genres = reuters.categories()\n",
    "words = ['croatian', 'player', 'deal', 'loan', 'barça', 'career', 'debut', 'palmas',  'birmingham']\n",
    "most_frequent_words_in_reuters = {}\n",
    "for genre in genres:\n",
    "    genre_text = reuters.words(categories = genre)\n",
    "    fdist = nltk.FreqDist(genre_text)\n",
    "    for w in words:\n",
    "        if fdist[w] > 0:\n",
    "            lst = []\n",
    "            lst.append(w)\n",
    "            lst.append(fdist[w])\n",
    "            most_frequent_words_in_reuters[genre] = lst\n",
    "\n",
    "print()\n",
    "for key in most_frequent_words_in_reuters:\n",
    "    print(key, '->', most_frequent_words_in_reuters[key])"
   ]
  },
  {
   "source": [
    "### Lingvističko stablo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'He set a string of records in the two seasons he spent in Dinamo’s first team'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_np = r\"NP: {<DT>?<CD>?<NN|NNS>}\" \n",
    "chunk_parser1 = nltk.RegexpParser(grammar_np)\n",
    "sent_tokens1 = nltk.pos_tag(word_tokenize(sentence))\n",
    "\n",
    "chunk_parser1.parse(sent_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_vp = r\"VP: {<PRP>?<VBP|VBN|VBD>*<NP|NNP>*<IN>*<NNP>?}\"\n",
    "chunk_parser2 = nltk.RegexpParser(grammar_vp)\n",
    "sent_tokens2 = nltk.pos_tag(word_tokenize(sentence))\n",
    "\n",
    "chunk_parser2.parse(sent_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}